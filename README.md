# WFMU Blog Archive Tool

A comprehensive archiving solution for preserving the WFMU blog (blog.wfmu.org) before it shuts down. This tool scrapes the entire blog history, downloads media files, parses content, creates a searchable archive, and provides a modern web interface for viewing the preserved content.

## ğŸš€ Quick Start

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Start archiving the blog
python scraper_paginated.py

# 3. View the archive
python wfmu_viewer_enhanced.py
# Open http://localhost:8080

# 4. View statistics
python archive_stats.py
# Open http://localhost:8085
```

## ğŸ“š How It Works

### Architecture Overview

The WFMU Archive system consists of several components working together:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 WFMU Blog Website                â”‚
â”‚              (blog.wfmu.org - source)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ Scraping
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           scraper_paginated.py                   â”‚
â”‚   - Fetches posts via pagination                 â”‚
â”‚   - Extracts metadata & content                  â”‚
â”‚   - Identifies media files                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ Stores
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           SQLite Database                        â”‚
â”‚         (wfmu_archive.db)                        â”‚
â”‚                                                  â”‚
â”‚  Tables:                                         â”‚
â”‚  - posts: Blog posts with content               â”‚
â”‚  - media: Images, audio, video files            â”‚
â”‚  - categories: Blog categories                  â”‚
â”‚  - comments: User comments                      â”‚
â”‚  - scrape_progress: Tracking                    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                      â”‚
       â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Viewer     â”‚      â”‚ Media Downloader â”‚
â”‚   (port 8080)â”‚      â”‚ download_images.pyâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Database Schema

The entire archive is stored in a SQLite database (`wfmu_archive.db`) with the following structure:

#### Posts Table
```sql
CREATE TABLE posts (
    post_id VARCHAR PRIMARY KEY,      -- MD5 hash of URL
    url VARCHAR UNIQUE NOT NULL,      -- Original blog URL
    title TEXT,                       -- Post title
    author VARCHAR,                   -- Author name
    published_date DATETIME,          -- Publication date
    raw_html TEXT,                    -- Original HTML
    content_text TEXT,                -- Extracted text
    content_markdown TEXT,            -- Markdown version
    categories TEXT,                  -- JSON array
    tags TEXT,                        -- JSON array
    scrape_date DATETIME              -- When scraped
)
```

#### Media Table
```sql
CREATE TABLE media (
    id INTEGER PRIMARY KEY,
    post_id VARCHAR,                  -- Links to posts.post_id
    media_type VARCHAR,               -- 'image', 'audio', 'video'
    original_url VARCHAR,             -- Source URL
    local_path VARCHAR,               -- Where file is saved
    filename VARCHAR,                 -- Local filename
    alt_text TEXT,                    -- Image alt text
    caption TEXT,                     -- Media caption
    downloaded BOOLEAN,               -- Download status
    download_error TEXT,              -- Error if failed
    FOREIGN KEY (post_id) REFERENCES posts(post_id)
)
```

## ğŸ”§ Components

### 1. Scraper (`scraper_paginated.py`)

The main scraper that archives the blog:

```bash
python scraper_paginated.py
```

**Features:**
- Uses pagination strategy (/page/2/, /page/3/, etc.)
- Respects rate limits (1 second between requests)
- Extracts all post metadata and content
- Identifies all media files
- Resumes from interruptions
- Handles errors gracefully

**How it works:**
1. Starts from the homepage
2. Follows pagination links to get all posts
3. For each post:
   - Downloads HTML
   - Extracts metadata (title, author, date)
   - Parses content to text/markdown
   - Identifies media files
   - Stores in database

### 2. Media Downloader (`download_images.py`)

Downloads image files respectfully:

```bash
# Test with 10 images
echo "test" | python download_images.py

# Download all images (be respectful!)
echo "yes" | python download_images.py
```

**Features:**
- Rate limiting (0.1s between images)
- Batch processing (100 images per batch)
- 10-second pause between batches
- Resume capability
- Progress tracking

**Current Status:**
- 10,710 total images identified
- 2,263+ downloaded (ongoing)
- ~8,447 remaining

### 3. Audio Downloader (`download_audio.py`)

Downloads MP3/audio files very respectfully (for future use):

```bash
# Test with 5 audio files
echo "test" | python download_audio.py

# Resume downloading where left off
echo "resume" | python download_audio.py

# Start full download (will take 3-4 days)
echo "yes" | python download_audio.py
```

**Features:**
- Very respectful rate limiting (2s between files)
- Batch processing (50 files per batch)
- 30-second pause between batches
- Full resume capability - won't re-download
- Progress tracking with size estimates
- File size display during download

**Current Status:**
- 10,666 total audio files identified
- 87 downloaded (637MB)
- 10,579 remaining
- Estimated ~77GB total storage needed
- Estimated 3-4 days continuous runtime

### 4. Web Viewer (`wfmu_viewer_enhanced.py`)

Modern web interface for browsing the archive:

```bash
python wfmu_viewer_enhanced.py
# Open http://localhost:8080
```

**Features:**
- **Live Search**: Instant results as you type
- **Browse by Date**: Year/month navigation
- **Browse by Author**: Filter by blog authors
- **Pagination**: 50 posts per page
- **Dual View**: Modern redesign or original HTML
- **Archive.org Links**: All external links use Wayback Machine

### 4. Statistics Dashboard (`archive_stats.py`)

Analytics and monitoring:

```bash
python archive_stats.py
# Open http://localhost:8085
```

**Shows:**
- Total posts archived
- Media file statistics
- Timeline visualization
- Author contributions
- Archive completeness

## ğŸ“Š Current Archive Status

As of the last run:
- **Posts**: 4,568 archived (2007-2015)
- **Images**: 8,147 identified, 58 downloaded
- **Audio**: 9,105 identified
- **Authors**: Various contributors
- **Date Range**: Missing 2003-2006 (pagination doesn't go that far back)

## ğŸ—ï¸ Building the Archive

### Step 1: Initial Scraping

```bash
# Start the scraper
python scraper_paginated.py

# This will:
# 1. Create wfmu_archive.db if it doesn't exist
# 2. Begin scraping from page 1
# 3. Continue through all pages
# 4. Extract posts, metadata, and identify media
# 5. Can be interrupted and resumed
```

### Step 2: Download Media (Optional)

```bash
# Download images (respectfully)
python download_images.py

# Choose:
# - "test" for 10 images
# - "yes" for all images
# - "no" to cancel
```

### Step 3: View the Archive

```bash
# Start the viewer
python wfmu_viewer_enhanced.py

# Features available at http://localhost:8080:
# - Search posts
# - Browse by year/month
# - Browse by author
# - View individual posts
# - Access Archive.org versions
```

### Step 4: Monitor Progress

```bash
# View statistics
python archive_stats.py

# Available at http://localhost:8085
```

## ğŸ—‚ï¸ File Structure

```
wfmu/
â”œâ”€â”€ wfmu_archive.db           # Main SQLite database
â”œâ”€â”€ wfmu_archive_viewer.db    # Copy for viewer (avoids locks)
â”œâ”€â”€ media/                    # Downloaded media files
â”‚   â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ audio/
â”‚   â””â”€â”€ video/
â”œâ”€â”€ templates/                # HTML templates
â”‚   â”œâ”€â”€ index.html           # Main viewer interface
â”‚   â””â”€â”€ post.html            # Individual post view
â”œâ”€â”€ static/                   # Static assets
â”‚   â”œâ”€â”€ css/style.css        # Styles
â”‚   â””â”€â”€ js/app.js            # JavaScript
â”œâ”€â”€ scraper_paginated.py     # Main scraper
â”œâ”€â”€ download_images.py       # Image downloader
â”œâ”€â”€ wfmu_viewer_enhanced.py  # Web viewer
â”œâ”€â”€ archive_stats.py         # Statistics dashboard
â”œâ”€â”€ database.py              # Database models (SQLAlchemy)
â””â”€â”€ requirements.txt         # Python dependencies
```

## ğŸ”„ Database Operations

### Check Archive Status
```python
import sqlite3
conn = sqlite3.connect('wfmu_archive.db')
cur = conn.cursor()

# Total posts
total = cur.execute("SELECT COUNT(*) FROM posts").fetchone()[0]
print(f"Total posts: {total}")

# Date range
dates = cur.execute("""
    SELECT MIN(published_date), MAX(published_date)
    FROM posts WHERE published_date IS NOT NULL
""").fetchone()
print(f"Date range: {dates[0]} to {dates[1]}")

# Media stats
media = cur.execute("""
    SELECT media_type, COUNT(*), SUM(downloaded)
    FROM media GROUP BY media_type
""").fetchall()
for m in media:
    print(f"{m[0]}: {m[1]} total, {m[2] or 0} downloaded")
```

### Export Data
```bash
# Export posts to JSON
python archive_wfmu.py export-json

# Creates archive_posts.json with all post data
```

### Backup Database
```bash
# Create backup
cp wfmu_archive.db wfmu_archive_backup_$(date +%Y%m%d).db
```

## âš ï¸ Important Notes

1. **Be Respectful**: The scraper includes delays to avoid overwhelming WFMU's servers
2. **Storage Requirements**: Full archive with media may require several GB
3. **Time Requirements**: Complete archiving may take several hours
4. **Incremental Updates**: The scraper can resume from interruptions
5. **Database Locking**: The viewer uses a copy of the database to avoid conflicts

## ğŸš¦ Troubleshooting

### Database Locked
```bash
# Create a copy for the viewer
cp wfmu_archive.db wfmu_archive_viewer.db
```

### Missing Posts
- The paginated scraper can only go back to around 2007
- Earlier posts (2003-2006) may require different approach

### Media Download Issues
- Check `download_error` field in media table
- Adjust rate limits if getting blocked
- Some media URLs may no longer be valid

### Viewer Not Showing Data
- Ensure database has data: `sqlite3 wfmu_archive.db "SELECT COUNT(*) FROM posts"`
- Check that `wfmu_archive_viewer.db` exists
- Restart the viewer after database updates

## ğŸ¯ Next Steps

1. **Complete Image Downloads**: 8,000+ images remaining
2. **Audio Files**: Plan strategy for 9,000+ MP3 files
3. **Missing Years**: Investigate accessing 2003-2006 posts
4. **Export Options**: Static site generation for permanent hosting

## ğŸ“ License

This archiving tool is provided for preservation purposes. Please respect the original content creators and WFMU's copyright. The archived content belongs to WFMU and its contributors.

## ğŸ™ Acknowledgments

Thanks to WFMU for decades of freeform radio and blog content. This archive ensures that the community's contributions are preserved for future generations.

---

**Remember**: Be a respectful archiver. Use appropriate delays and don't overwhelm the source servers.